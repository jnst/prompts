---
topic: "Transformerアーキテクチャ"
prompt_version: "1.0.0"
timestamp: "2025-06-29T00:00:00"
---

Transformerアーキテクチャを **「翻訳者の会議室」** として理解しましょう。

### 核心：並列的な相互参照システム

従来のRNN/LSTMが「一人の翻訳者が順番に単語を処理」していたのに対し、Transformerは「全員が同時に全体を見ながら議論する会議」です。

#### なぜこの設計なのか？

**問題**：「I saw a cat」を「私は猫を見た」に翻訳する時、「saw」の意味は文末の「cat」を見ないと確定できない。順次処理では遠い単語の関係を見失いやすい。

**解決**：Self-Attention（自己注意機構）= 全単語が互いの重要度を同時に計算

```
各単語が問う：「私の意味を決めるのに、他のどの単語がどれくらい重要？」
→ Attention Weight（注意の重み）として数値化
```

#### 3つの重要な「なぜ」

1. **なぜMulti-Head（複数の視点）？**
   - 「bank」は「river bank」なら地形、「bank account」なら金融
   - 複数のAttention Headが異なる関係性（文法、意味、文脈）を同時に捉える
2. **なぜPositional Encoding（位置情報の付加）？**
   - 並列処理では語順が失われる
   - 「犬が猫を追いかけた」vs「猫が犬を追いかけた」を区別するため
3. **なぜEncoderとDecoderの分離？**
   - Encoder：入力を深く理解する（文脈の圧縮）
   - Decoder：出力を生成する（圧縮された理解から展開）
   - 中間の表現が他タスクに転用可能に

#### 応用と関連付け

このアーキテクチャは**汎用的な関係性学習器**として機能するため：

- **言語**：GPT（文章生成）、BERT（文章理解）
- **画像**：Vision Transformer（画像をパッチの系列として処理）
- **音声**：Whisper（音声認識）
- **マルチモーダル**：CLIP（画像と言語の関係学習）

すべて「要素間の関係性をAttentionで学習する」という同じ原理。

#### 記憶のアンカー

**「Transformerとは、全要素が並列に相互参照し合うことで、文脈を理解する仕組み」**

この一文から、なぜ長距離依存に強いか、なぜ並列計算が可能か、なぜ様々な分野に応用できるかが導き出せます。
